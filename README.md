# â¬¡ Neural Lab v5

> Simulateur pÃ©dagogique interactif de rÃ©seaux de neurones profonds â€” 100% navigateur, zÃ©ro dÃ©pendance.

![Neural Lab v5](https://img.shields.io/badge/version-v5-00e5ff?style=flat-square&labelColor=0a0c18)
![HTML](https://img.shields.io/badge/HTML-single%20file-bf5fff?style=flat-square&labelColor=0a0c18)
![License](https://img.shields.io/badge/license-MIT-00ff9d?style=flat-square&labelColor=0a0c18)
![No dependencies](https://img.shields.io/badge/deps-zero-ffd600?style=flat-square&labelColor=0a0c18)

---

## âœ¨ PrÃ©sentation

**Neural Lab** est un simulateur visuel et pÃ©dagogique de rÃ©seaux de neurones artificiels (MLP â€” Multi-Layer Perceptron). Il expose **tous les calculs internes** Ã©tape par Ã©tape : forward pass, backpropagation, descente de gradient, mise Ã  jour des poids â€” le tout en temps rÃ©el, dans votre navigateur, sans aucune installation.

Un seul fichier HTML. Ouvrez-le, c'est parti.

---

## ğŸ–¥ï¸ DÃ©mo

```
git clone https://github.com/votre-pseudo/neural-lab
cd neural-lab
# Ouvrir neural_lab_v5.html dans votre navigateur
```

Ou simplement double-cliquer sur `neural_lab_v5.html`.

---

## ğŸš€ FonctionnalitÃ©s

### Architecture & construction
- **Constructeur simple** â€” dÃ©finir les couches via une chaÃ®ne (`2,4,3,1`)
- **â¬¡ Constructeur avancÃ©** â€” configurer chaque couche individuellement :
  - Nombre de neurones (slider + input)
  - Fonction d'activation propre par couche
  - Fiches d'info par activation (Ã©quation, plage, avertissements vanishing gradient)
  - DÃ©placer / supprimer les couches cachÃ©es
  - Preview en temps rÃ©el du rÃ©seau
  - 6 prÃ©rÃ©glages rapides (XOR, Deep, Autoencodeur, Classif 3 classes, RÃ©gression, Large)
- **Wizard** â€” configuration guidÃ©e par type de problÃ¨me (classification binaire, multiclasse, rÃ©gression)

### Fonctions d'activation (10)
`Sigmoid` Â· `ReLU` Â· `Tanh` Â· `Leaky ReLU` Â· `ELU` Â· `Swish/SiLU` Â· `GELU` Â· `SELU` Â· `Softsign` Â· `LinÃ©aire` Â· `Softmax`

### Fonctions de perte (5)
`MSE` Â· `MAE` Â· `Log Loss (BCE)` Â· `Huber` Â· `Hinge`

### Optimiseurs (6)
`SGD` Â· `Momentum` Â· `Nesterov AG` Â· `RMSProp` Â· `Adam` Â· `AdamW`

### EntraÃ®nement
- **Step** â€” un seul exemple alÃ©atoire
- **+Epoch** â€” une Ã©poque complÃ¨te sur le dataset
- **Ã—100 / Ã—1k / Ã—10k** â€” entraÃ®nement rapide en masse
- **Auto** â€” boucle continue avec vitesse rÃ©glable
- Courbe de loss en temps rÃ©el (clic pour effacer)
- Affichage Epoch / Loss / Accuracy / LR courant

### Visualisation canvas
- RÃ©seau entiÃ¨rement dessinÃ© en Canvas 2D
- Ã‰paisseur des connexions âˆ valeur absolue du poids
- Couleur des connexions : vert = positif, rouge = nÃ©gatif
- Glow des neurones âˆ valeur d'activation
- Ã‰tiquettes : poids, biais, activations, indices
- **LÃ©gende** : entrÃ©e/cachÃ©/sortie + code couleur des connexions
- Tooltips au survol (neurone : a, z, b, activation fn ; connexion : w, type)
- Clic sur un neurone ou une connexion â†’ panneau de dÃ©tail complet

### Panneau de dÃ©tail (onglet DÃ©tails)
- Pour chaque **neurone** : index, activation fn, valeur z, valeur a, biais, tableau des poids entrants avec wÃ—a, calcul complet z = Î£(wáµ¢Ã—aáµ¢) + b, dÃ©rivÃ©e f'(z), gradient local Î´ avec dÃ©tection vanishing/exploding
- Pour chaque **connexion** : poids w, signal propagÃ© wÃ—a, gradient âˆ‚L/âˆ‚w, calcul de la mise Ã  jour

### Log des calculs
- 4 niveaux de verbositÃ© : Complet / Moyen / Par epoch / Silencieux
- Mode **Complet** : chaque opÃ©ration est dÃ©taillÃ©e (z, a, termes de la loss, deltas backprop, mises Ã  jour)
- Copier le log dans le presse-papier

### BibliothÃ¨que de formules (onglet Formules)
- 30+ formules rÃ©fÃ©rencÃ©es : activations, pertes, optimiseurs, backprop, rÃ©gularisation, mÃ©triques, initialisation, architectures
- **LÃ©gende globale** des symboles mathÃ©matiques (z, a, w, b, Î´, âˆ‚L/âˆ‚w, lr, Å·, y, Ïƒ, Î²â‚/Î²â‚‚, Î», Î£, f'(z)...)
- **LÃ©gende contextuelle** par formule (symboles pertinents uniquement)
- Graphe de la fonction pour chaque activation
- Recherche full-text + filtrage par tag
- Avantages / inconvÃ©nients pour chaque formule

### Onglet Test
- Saisir des valeurs d'entrÃ©e arbitraires
- PrÃ©diction dÃ©taillÃ©e avec forward pass pas-Ã -pas
- Test complet du dataset avec accuracy par exemple

### Dataset
- 7 prÃ©rÃ©glages : XOR, AND, OR, NAND, XNOR, Half Adder, 4-bit Identity
- Ã‰diteur de dataset visuel (ajouter/supprimer lignes)
- Import/Export JSON
- Dimensions configurables (n entrÃ©es Ã— m sorties)

### Options avancÃ©es (onglet Options)
| CatÃ©gorie | ParamÃ¨tres |
|---|---|
| ThÃ¨me & couleurs | 6 thÃ¨mes (Dark, Neon, Ocean, Fire, Matrix, Pastel) + couleurs custom |
| GÃ©omÃ©trie | Rayon neurones, Ã©paisseur connexions, taille police, espacement H/V |
| Effets visuels | Glow, opacitÃ©, BÃ©zier, flÃ¨ches, couleur par valeur, grille, lÃ©gende |
| Algorithme | Init poids (6 mÃ©thodes), momentum Î², Adam Î²â‚/Î²â‚‚/Îµ, L2 Î», Dropout, Huber Î´, gradient clipping, LR decay |
| Sauvegarde | Export JSON rÃ©seau, Import JSON, Export code Python, Export code JS |

### Redimensionnement des panneaux (v5)
- **Panneau gauche** : glisser la barre verticale (min 160px / max 520px)
- **Panneau droit** : glisser la barre verticale (min 200px / max 650px)
- **Log bas** : glisser la barre horizontale (min 60px / max 70vh)
- Double-clic sur une barre = reset aux dimensions par dÃ©faut

---

## ğŸ“ Algorithmes implÃ©mentÃ©s

### Forward pass
```
z[l,j] = Î£(w[l,j,k] Ã— a[l-1,k]) + b[l,j]
a[l,j] = f(z[l,j])
```

### Backpropagation (rÃ¨gle de la chaÃ®ne)
```
Î´[L,j] = âˆ‚L/âˆ‚a Â· f'(z)          â† couche sortie
Î´[l,k] = (Î£ Î´[l+1,j] Â· w[l+1,j,k]) Â· f'(z)   â† couches cachÃ©es
âˆ‚L/âˆ‚w[l,j,k] = Î´[l,j] Â· a[l-1,k]
âˆ‚L/âˆ‚b[l,j]   = Î´[l,j]
```

### Adam (exemple)
```
m â† Î²â‚Â·m + (1âˆ’Î²â‚)Â·g
v â† Î²â‚‚Â·v + (1âˆ’Î²â‚‚)Â·gÂ²
mÌ‚ = m/(1âˆ’Î²â‚áµ—)    vÌ‚ = v/(1âˆ’Î²â‚‚áµ—)
w â† w âˆ’ lrÂ·mÌ‚/(âˆšvÌ‚ + Îµ)
```

---

## ğŸ—‚ï¸ Structure du projet

```
neural-lab/
â””â”€â”€ neural_lab_v5.html    # Application complÃ¨te (fichier unique)
```

C'est volontairement un fichier unique â€” aucune dÃ©pendance externe sauf deux polices Google Fonts (`JetBrains Mono`, `Syne`).

---

## ğŸ’¡ ExpÃ©riences recommandÃ©es

| ExpÃ©rience | Config suggÃ©rÃ©e |
|---|---|
| XOR non-linÃ©airement sÃ©parable | `2,4,1` Â· Sigmoid Â· lr=0.5 Â· Adam |
| Vanishing gradient | `2,8,8,8,8,1` Â· Sigmoid Â· log Complet |
| ReLU vs Sigmoid (profond) | MÃªme archi, changer activation, comparer loss |
| Divergence par lr trop Ã©levÃ© | lr=5 Â· SGD |
| Adam vs SGD | MÃªme rÃ©seau + dataset, changer optimiseur |
| Autoencodeur | `4,2,4` Â· 4-bit Identity dataset |
| Activations mixtes | Constructeur avancÃ© : ReLU cachÃ©es + Sigmoid sortie |
| RÃ©gularisation L2 | Options â†’ Î»=0.01, observer les poids |

---

## ğŸ¨ ThÃ¨mes

| ThÃ¨me | Ambiance |
|---|---|
| **Dark** | DÃ©faut â€” bleu nuit profond |
| **Neon** | Cyberpunk saturÃ© |
| **Ocean** | Bleu profond |
| **Fire** | Orange/rouge chaud |
| **Matrix** | Vert terminal |
| **Pastel** | Doux, lisible |

---

## ğŸ“¦ Export

Le rÃ©seau entraÃ®nÃ© peut Ãªtre exportÃ© en :
- **JSON** â€” sauvegarde complÃ¨te (poids, biais, historique de loss, activations par couche)
- **Python (NumPy)** â€” code fonctionnel prÃªt Ã  l'emploi
- **JavaScript** â€” code `predict()` autonome

---

## ğŸ› ï¸ Technologies

- **HTML5 Canvas** â€” rendu du rÃ©seau et courbe de loss
- **Vanilla JS ES6+** â€” zÃ©ro framework, zÃ©ro build
- **CSS custom properties** â€” theming complet
- **Google Fonts** â€” JetBrains Mono Â· Syne

---

## ğŸ“– Ressources pÃ©dagogiques

Ce simulateur couvre visuellement :
- Le thÃ©orÃ¨me d'approximation universelle
- Le problÃ¨me du vanishing/exploding gradient
- L'effet du learning rate sur la convergence
- L'initialisation des poids (Xavier, He, LeCun...)
- La rÃ©gularisation (L2, Dropout)
- Les diffÃ©rentes familles d'optimiseurs

---

## ğŸ“„ Licence

MIT â€” libre d'utilisation, modification et distribution.

---

<p align="center">
  Fait avec â¬¡ pour apprendre le deep learning de l'intÃ©rieur.
</p>
